---
layout: post
title: Video Kill the Mocap System?
date: 2026-02-14 14:00:16
description: Video to Motion에 대한 생각
tags: blog_kr, thoughts
categories: blog_kr
_styles: |
  #markdown-content {
    line-height: 1.8;
  }
  #markdown-content p {
    margin: 1.1em 0;
  }
---
[English Version](https://pulsekim.github.io/blog_en/2026/2026-0213-VideoKillTheMocapStar/)

나의 블로그가 늘 그렇듯, 이 글 역시 정리가 덜 된 글이다. 누군가에게는 공감되지 않는 이야기일 수도 있다.

오늘은 과감하게 Video to Motion에 대한 이런저런 생각을 적어보려고 한다. 이 분야는 연구하는 사람도 꽤 많아서 조심스럽긴 하지만, 시작해본다.

사실 지난 포스트에서 언급했듯이, 회사에서도 이 분야를 한 번 시도해보려고 했고 많은 애니메이터들과 이야기를 나눠본 경험이 있다. 상황상 프로젝트를 접은 지도 꽤 시간이 지났고, confidential한 부분은 알아서 걸러서 쓸 예정이니 몇 가지 아이디어 정도는 공유해도 괜찮을 것 같다.


## Preliminary
흠… 이걸 follow-up하지 않은 지도 꽤 되었고, 솔직히 이 연구를 마지막으로 살펴본 건 2024년 SIGGRAPH Asia의 [GVHMMR](https://zju3dv.github.io/gvhmr/)과 2025년 CVPR의 [MVLIFT](https://lijiaman.github.io/projects/mvlift/) 정도다. 혹시 아래에서 열심히 이야기한 문제들이 이미 다른 연구에서 해결된 부분이라면, 그냥 생각 덤프라고 생각하고 가볍게 읽어주면 좋겠다.

## Can Video Kill the Mocap System?
모션 연구를 하다 보면 금방 느끼는 점이 하나 있다. 사용할 수 있는 데이터셋이 매우 제한적이라는 것이다. 일부 회사들을 제외하면 public으로 공개된 데이터는 많지 않다. 조금 아쉬운 이야기지만, VFX 회사나 게임 회사, AI 회사에서 인하우스 모캡 데이터를 공개하는 경우는 거의 없다. 대부분의 회사 데이터는 아카데미아에 공개된 데이터보다 훨씬 높은 퀄리티를 가지며, 만약 공개된다면 연구자 입장에서는 정말 큰 도움이 될 것이다.

그렇다고 그들을 비난할 수는 없다. 오히려 이해가 되는 부분이 많다. 제스처 연구를 진행하면서 모캡 시나리오 작성과 디렉팅을 약 1년 정도 경험했고, 처음에는 배우를 구하지 못해 직접 연기까지 해본 적도 있다.

좋은 광학식 모캡 데이터를 얻기 위해서는 수십 대의 특수 카메라가 설치된 스튜디오가 필요하고, 전문 액션 배우를 고용해 촬영해야 한다. 촬영 시나리오도 정교하게 준비해야 하며, 촬영 중에는 수많은 NG가 발생한다. 하루 종일 촬영해도 실제로 사용할 수 있는 컷은 많지 않다. 촬영이 끝난 뒤에도 전문가들이 며칠에서 일주일 이상 클린업 작업을 해야 비로소 사용할 수 있는 데이터가 된다. 인력과 비용이 모두 많이 드는 작업이다. 또한 배우 선정, 디렉팅, 시나리오 구성 등 회사의 노하우가 모캡 데이터에 녹아 있기 때문에 공개하기 어렵다는 점도 충분히 이해할 수 있다.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mocap.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    이런 쫄쫄이를 입고 하루 종일 촬영하면 배우도 지치기 때문에 오래 촬영이 불가하다. 물론 사진속 사람은 누군지는 ** 절대 모르는 사람** 이다. 근데 좀 잘생긴거 같다.
</div>

공개된 데이터셋의 상당수는 CMU나 SFU 같은 학교에서 제공한 것이다. 배우 대신 학생이 연기하거나, 전문 클린업 대신 학생이나 파트타임 엔지니어가 작업한 경우도 많다고 들었다. 실제로 CMU 데이터를 보면 다리 모션이 깨진 경우도 있고 퀄리티가 아쉬운 부분도 있다. 그럼에도 불구하고, 모캡 스튜디오 운영 비용과 노력을 생각하면 이 정도 데이터가 공개되어 있다는 것만으로도 매우 감사한 일이다.

그래서 자연스럽게 더 저렴하게 모션 데이터를 얻는 방법에 관심이 가게 된다. 가장 단순한 아이디어는 비디오에서 모션을 추출하는 것이다. 접근 가능한 데이터 양을 생각하면 매우 매력적인 방향이다. 사람은 비디오만 보고도 동작을 따라 할 수 있으니, 모델이나 최적화 방법으로도 비디오에서 모션을 추출할 수 있지 않을까 하는 생각이 든다.

나아가서 비디오에서 모션을 추출할 수 있다면 현재 있는 광학식 모캡의 종말은 올까? 내가 정말 좋아하는 The Buggles의 곡 [Video Kill the Radio Star](https://www.youtube.com/watch?v=W8r-tXRLazs)에서 영감을 얻어서 이 글의 제목을 지어봤다. 

Can Video Kill the Mocap System?

## Actually, video is a good source of motion
사실 비디오는 이미 애니메이터들이 많이 사용하는 모션 참고 자료다. [유튜브 강의](https://www.youtube.com/watch?v=UkWnwHwMapQ)나 레퍼런스 영상도 쉽게 찾을 수 있다. 다만 자동화된 방식이 아니라, 영상을 참고해 키 포즈(혹은 블로킹 포즈)를 직접 만들고 커브를 조정하는 방식이다. 회사에서 애니메이터 인터뷰를 진행했을 때도 많은 애니메이터가 비디오를 참고한다고 말했다. 그렇다면 왜 video-to-motion 연구나 툴은 실제 현업에서 널리 사용되지 않을까? 결국 문제는 퀄리티다. 

여러 애니메이터들에게 video-to-motion 연구로 나온 모션을 들고 가서 물어보면 결과물의 물리적인 특성이 많이 무너지고, 이를 수정하는 작업이 결국 손으로 키를 잡는 것과 크게 다르지 않다고 한다. 사실 video to motion 연구 논문들 결과를 읽다 보면 놀라는 경우가 많다. 논문을 보면 조인트 에러가 수cm~mm까지 줄어들었고, 벤치마크 숫자로 보기에는 이보다 발전하는게 큰 의미가 없어보이기까지 한다. 여기서 눈치 빠른 사람들은 내가 뭘 말하려는지 알았을거다. 벤치마크 숫자와 실제 사용성 사이에는 여전히 큰 간극이 있다. 즉, 벤치마크가 현실을 충분히 반영하지 못하는 경우가 많다.

## Dilema of Datasets
이 분야의 많은 문제는 데이터셋의 딜레마에서 시작된다. 우리가 비디오에서 얻고 싶은 모션은 공중 회전이나 빙판 위 동작처럼 모캡으로 촬영하기 어려운 동작들이다. 하지만 그런 동작일수록 비디오-모션 쌍 데이터를 만들기가 더 어렵다. 또한 학습 페어로 보통 요구되는게 비디오-모션 쌍일텐데, Quantity/Quality issue가 있다. 예컨데 널리 쓰이는 [Human3.6](http://vision.imar.ro/human3.6m/description.php)은 온통 새빨간 카펫이 깔린 환경에서 일상적인 동작만 촬영되었다(물론 가상 아바타로 데이터를 늘리긴 했다.). [EMDB](https://eth-ait.github.io/emdb/)는 다양한 씬에서 굉장히 다양한 동작을 하려고 했지만, 매우 역동적이고 빠른 동작을 하기 보다 데이터셋의 안정성을 위해서 보통 느릿한 동작들 위주로 구성되어 있다.일부 데이터셋은 IMU 기반이라 정확도 측면에서 trade-off가 존재한다.

물론 이 데이터셋들을 구축하는 것이 얼마나 어려운지 알기 때문에 그 노고에 대해 항상 감사하게 생각한다. 항상 이런 데이터를 찍고 아카데미아에 공유하는 그들의 노고에는 늘 감사하고 늘 존경한다. 그들의 방법론에 대한 비판은 절대 아니다. 그들이 제시한 방법은 그때마다 SOTA의 방법, 최선의 방법의 최선의 데이터셋이었을 것이며 존중해야한다. 다만 우리가 사용하는 데이터가 어떤 특성을 가지는지 이해해야, 왜 in-the-wild video-to-motion이 아직 충분한 퀄리티에 도달하지 못했는지를 설명할 수 있다. 결국 video-motion paired dataset은 quantity와 quality 모두에서 현실이 요구하는 수준보다 부족하고, 이것이 연구의 성능을 데이터셋으로 bound하는 요인이 된다.

## When Graphics meets Vision
나는 여러 분야가 섞이는게 매우 좋다. 항상 내가 대표적으로 언급하는게 video to motion이다. 이 분야는 비전 연구에서 먼저 발전했고, 실제로 영상 입력을 다루는 문제는 비전 연구자들이 더 빠르게 풀어왔다. 우리같은 그래픽스 전공자가 비전 입력 소스를 다루는 것보다 그들이 하는 편이 더 효율적이고 수월했다. Angjoo Kanzawa 교수님을 필두로 여러 유능한 비전 랩에서 이 분야를 먼저 풀려고 시도했고, pure vision 방법으로도 꽤 많은 성과를 냈다. 하지만 비전 접근만으로는 모션에 대한 domain knowledge가 충분히 반영되지 않는 경우가 많다. 나는 항상 이런 류의 논문을 볼 때 모션 쪽의 domain knowledge를 조금 첨가하면 훨씬 좋은 결과를 낼 수 있을 것 같다는 생각은 했지만 뚜렸한 아이디어는 없었다. 

그런 의미에서 [WHAM](https://wham.is.tue.mpg.de/) 논문에서 재밌는 생각을 제안했다. Motion prior를 이용하는것이다. 기본적인 아이디어는 모캡 데이터가 비디오-모션 데이터 셋보다는 퀄리티도 낫고, 양도 더 많으니 그걸 이용하자는 거다. 모션 캡처 데이터의 키 조인트를 2D로 찍은 뒤에 그걸 다시 3D로 lifting 하는 네트워크를 학습하는게 이 연구의 아이디어다. 물론 여러 아이디어가 더 포함된 좋은 논문이니 읽어보면 좋겠지만, 가장 좋았던것은 그래픽스쪽에서 다루는 모션 데이터를 갖고 비전에서 풀던 문제와 잘 융화시킨 부분에 있다. 모쪼록 두 분야의 장점을 계속 합하여 나가는 것이 지향되어야 할 방향이다.

## What matters to the quality?
이런 노력에도 아직 이 기술이 정말 "쓸만한 단계"까지 오지는 못햇다. 결국 지겹도록 이야기하는 퀄리티가 문제다. 애니메이터들이 이걸 쓰게 만드려면 어케해야 할까? 내 경험상 몇가지 요소들이 풀려야 한다. 

첫째로 발 미끌림이다. 많은 연구에서 contact probability 등을 이용하여 soft constraint로 이를 해결하려 했지만 사실 아직도 많이 개선해야 한다. 또한 미끌려야 할 때와 땅에 붙어야 할 때를 구별하는 것도 또 다른 이슈다. 이런 [스턴트영상](https://www.youtube.com/watch?v=8mRvRbBJOTo)에서 극단적으로 보이듯 어떤 움직임은 미끄러지는 동작을 포함한다. 무술에서 발을 끄는 동작이 있듯, 실제로 미끌려야 할 때와 그렇지 않을 때가 명명백백히 구별되어야 한다. 

Root의 움직임도 매우 큰 이슈이다. 간혹 video to motion연구 데모 영상을 보면 카메라 프레임(영상을 따라가는 앵글)에서는 매우 훌륭하게 모션이 나오는 "것처럼" 보인다. 하지만 이를 실제로 사용하기 위해서는 global coordinate에서의 루트의 움직임이 정말 중요하다. 이는 단순히 root의 global transition, heading angle의 문제만이 아니다.(물론 이 부분도 아직도 갈 길이 멀다). 카메라 기준 z축 방향의 각을 reconstruction 하는 것은 ill-posed problem이며 많은 경우 이 부분이 퀄리티에 영향을 준다. [GVHMR](https://zju3dv.github.io/gvhmr/)이 이 문제를 꽤 효과적으로 풀기는 했으니 관심 있는 사람은 봐도 좋다. 

회전 운동의 뉘앙스도 많이 뭉개지는것도 문제이다. 아무래도 빠르게 돌거나 피벗 발을 두고 발차기를 하는 모션이 적다보니 물리적으로 그럴듯한 회전을 리컨하는게 무척 어렵다. 이 부분은 사실 우리같은 일반인이 봤을 때 이질감은 느껴지지만 뭐가 문제인지 모르는 이슈였는데, 애니메이터 분들의 도움으로 회전 운동에 대한 물리적인 부분이 매우 부자연스러운 문제임을 알아냈다. 

또한 데이터 out-of-distribution이슈도 많은 영향을 미친다. 고품질 모션 데이터는 상대적으로 그 양이 적고, 비디오-모션 쌍으로 된 데이터는 더더욱 적다. 또한 위에서 말한 이슈로 인하여 데이터 밖에 있는 샘플은 매우 얻기가 힘들며, 이런 이슈로 인해 학습에 어려움을 겪기도 한다. 이 문제는 재작년 즈음 Karen liu 교수님과 한 번 개인적으로 이야기 해 본적이 있었는데, 나중에 [Lifting Motion to the 3D World via 2D Diffusion](https://lijiaman.github.io/projects/mvlift/)연구에서 정말 상상하지도 못한 멋진 아이디어를 보여주셨다. 

파생되는 문제 중에 모션을 부드럽게 만드려면 모션의 디테일이 다 죽어버리고, 반대의 경우 튀는 모션이 발생하는 trade-off도 골치 아픈 문제다. 마지막으로 비디오 데이터가 주로 30fps인것도 문제이다. Daniel holden이 말했듯, 모션은 최소 60Hz는 되어야 최소한의 디테일을 볼 수 있다. 퀄리티 측면에서 30fps는 치명적이다. 

이런 문제들은 한동안 나 혼자 생각해 보던 것들이었고, 언젠가 논문으로 정리해볼까도 고민했지만 요즘은 다른 연구에 집중하느라 깊게 파고들지는 못하고 있다. 그래도 누군가 이 문제를 해결해 준다면 나 역시 그 결과를 활용할 수 있을 것 같아 이렇게 적어본다.

## 맺으며
Video kill the mocap system? 내 마음 속의 대답은 "아직은 No고 미래에도 No다." 아무리 video to motion이 발전해도, 광학식 모캡의 고퀄리티 데이터의 메리트는 항상 있다. 다만, 언젠가는 video to motion이 충분히 고려해볼만한 대체제가 될 수 있다고 나는 굳게 믿는다. 아직은 모션 소스를 얻는 대체제의 기능을 못한다고 생각하지만 연구가 꽤 근접해 가고 있다고 생각한다. 

혹시 위에서 말한 아이디어로 연구를 해보고 싶은 사람이 있다면 언제든 열려 있다. 같이 연구해도 좋고, 아니어도 이 글에서 영감을 얻어 좋은 결과가 나온다면 그것만으로도 기쁠 것 같다. 나중에라도 그런 연구를 알게 된다면 꽤 뿌듯할 것 같다.


#### 출처
The buggles - Video kill the radio star, https://www.youtube.com/watch?v=W8r-tXRLazs
How to use video reference for animatio, https://www.youtube.com/watch?v=UkWnwHwMapQ 
Motion Actor Inc, https://www.youtube.com/@MotionActorInc 
Game-like motion in a business suit.  https://www.youtube.com/watch?v=8mRvRbBJOTo
WHAM https://wham.is.tue.mpg.de/
GVHMR https://zju3dv.github.io/gvhmr/
MVLIFT https://lijiaman.github.io/projects/mvlift/
