+++
weight = 1
author = "Sunwoo Kim"
title = "Body Gesture Generation for Multimodal Conversational Agents"
date = "2024-10-01"
description = "BodyGesture"
tags = [
    "Publications","Computer Animation", "Adversarial Learning", "Multimodal Agent", "Gesture"
]
feature_image = "/images/papers_725s3.jpg"
+++
Siggraph Asia, 2024
<!--more-->
### Authors
Sunwoo Kim, Minwook Chang, Yoonhee Kim, Jehee Lee

### Project Site
[Project Site](https://sites.google.com/view/bodygesture4multimodal)
[Paper](https://dl.acm.org/doi/10.1145/3680528.3687648)
[Video](https://www.youtube.com/watch?v=yVvaonY_F1I&ab_channel=Sunwoo_PulseKim)

### Abstract
Creating intelligent virtual agents with realistic conversational abilities necessitates a multimodal communication approach extending beyond text. Body gestures, in particular, play a pivotal role in delivering a lifelike user experience by providing additional context, such as agreement, confusion, and emotional states. This paper introduces an integration of motion matching framework with a learning-based approach for generating gestures, suitable for multimodal, real-time, interactive conversational agents mimicking natural human discourse. Our gesture generation framework enables accurate synchronization with both the rhythm and semantics of spoken language accompanied by multimodal perceptual cues. It also incorporates gesture phasing theory from social studies to maintain critical gesture features while ensuring agile responses to unexpected interruptions and barging-in situations. Our system demonstrates responsiveness, fluidity, and quality beyond traditional turn-based gesture-generation methods.